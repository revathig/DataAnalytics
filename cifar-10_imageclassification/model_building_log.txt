from keras.datasets import cifar10

import cv2

import numpy as np

from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

from keras.models import Model, Sequential
from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation
from keras.constraints import maxnorm
from keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import BatchNormalization
import os
import keras



(trainX, trainy), (testX, testy) = cifar10.load_data()

print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))
print('Test: X=%s, y=%s' % (testX.shape, testy.shape))


from matplotlib import pyplot
# plot first few images
for i in range(9):
    pyplot.subplot(330 + 1 + i)
    # plot raw pixel data
    pyplot.imshow(trainX[i])

# show the figure
pyplot.show()


# Convert class vectors to binary class matrices.
num_classes =10
trainy = keras.utils.to_categorical(trainy, num_classes)
testy = keras.utils.to_categorical(testy, num_classes)

##Preprocess data
trainX = trainX.astype('float32')
trainX/=255

testX = testX.astype('float32')
testX/=255


input_shape = trainX.shape[1:]
input_shape


###Data Augmentation

from keras.preprocessing.image import ImageDataGenerator
BATCH_SIZE = 64

# Create train generator.
train_datagen = ImageDataGenerator(rotation_range=10, 
                                   width_shift_range=0.1,
                                   height_shift_range=0.1, 
                                   horizontal_flip = 'true')

### model bi=uilding

##1st model without augmentation and transfer learning

model = Sequential()

model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32,32,3)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))


model.add(Conv2D(128, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Conv2D(128, (3, 3)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes))
model.add(Activation('softmax'))

# initiate RMSprop optimizer
opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)

# Let's train the model using RMSprop
model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

#opt = SGD(lr=0.001, momentum=0.9)
#model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

##model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=["accuracy"])

model.summary()

train_datagen.fit(trainX)


hist = model.fit_generator(train_datagen.flow(trainX, trainy,
                                     batch_size=64), epochs=110, validation_data=(testX, testy), workers = 4)
Train: X=(50000, 32, 32, 3), y=(50000, 1)
Test: X=(10000, 32, 32, 3), y=(10000, 1)

ï¿¼
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_46 (Conv2D)           (None, 30, 30, 32)        896       
_________________________________________________________________
activation_9 (Activation)    (None, 30, 30, 32)        0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 30, 30, 32)        128       
_________________________________________________________________
conv2d_47 (Conv2D)           (None, 28, 28, 32)        9248      
_________________________________________________________________
activation_10 (Activation)   (None, 28, 28, 32)        0         
_________________________________________________________________
batch_normalization_8 (Batch (None, 28, 28, 32)        128       
_________________________________________________________________
max_pooling2d_23 (MaxPooling (None, 14, 14, 32)        0         
_________________________________________________________________
dropout_21 (Dropout)         (None, 14, 14, 32)        0         
_________________________________________________________________
conv2d_48 (Conv2D)           (None, 14, 14, 64)        18496     
_________________________________________________________________
activation_11 (Activation)   (None, 14, 14, 64)        0         
_________________________________________________________________
batch_normalization_9 (Batch (None, 14, 14, 64)        256       
_________________________________________________________________
conv2d_49 (Conv2D)           (None, 12, 12, 64)        36928     
_________________________________________________________________
activation_12 (Activation)   (None, 12, 12, 64)        0         
_________________________________________________________________
batch_normalization_10 (Batc (None, 12, 12, 64)        256       
_________________________________________________________________
max_pooling2d_24 (MaxPooling (None, 6, 6, 64)          0         
_________________________________________________________________
dropout_22 (Dropout)         (None, 6, 6, 64)          0         
_________________________________________________________________
conv2d_50 (Conv2D)           (None, 6, 6, 128)         73856     
_________________________________________________________________
activation_13 (Activation)   (None, 6, 6, 128)         0         
_________________________________________________________________
batch_normalization_11 (Batc (None, 6, 6, 128)         512       
_________________________________________________________________
conv2d_51 (Conv2D)           (None, 4, 4, 128)         147584    
_________________________________________________________________
activation_14 (Activation)   (None, 4, 4, 128)         0         
_________________________________________________________________
batch_normalization_12 (Batc (None, 4, 4, 128)         512       
_________________________________________________________________
max_pooling2d_25 (MaxPooling (None, 2, 2, 128)         0         
_________________________________________________________________
dropout_23 (Dropout)         (None, 2, 2, 128)         0         
_________________________________________________________________
flatten_4 (Flatten)          (None, 512)               0         
_________________________________________________________________
dense_7 (Dense)              (None, 512)               262656    
_________________________________________________________________
activation_15 (Activation)   (None, 512)               0         
_________________________________________________________________
dropout_24 (Dropout)         (None, 512)               0         
_________________________________________________________________
dense_8 (Dense)              (None, 10)                5130      
_________________________________________________________________
activation_16 (Activation)   (None, 10)                0         
=================================================================
Total params: 556,586
Trainable params: 555,690
Non-trainable params: 896
_________________________________________________________________
Epoch 1/110
782/782 [==============================] - 1205s 2s/step - loss: 2.1594 - acc: 0.2706 - val_loss: 1.5994 - val_acc: 0.4174
Epoch 2/110
782/782 [==============================] - 1153s 1s/step - loss: 1.6378 - acc: 0.4016 - val_loss: 1.4654 - val_acc: 0.4743
Epoch 3/110
782/782 [==============================] - 1174s 2s/step - loss: 1.4864 - acc: 0.4632 - val_loss: 1.5723 - val_acc: 0.4698
Epoch 4/110
782/782 [==============================] - 1171s 1s/step - loss: 1.3818 - acc: 0.5037 - val_loss: 1.3227 - val_acc: 0.5445
Epoch 5/110
782/782 [==============================] - 1191s 2s/step - loss: 1.2998 - acc: 0.5342 - val_loss: 1.3715 - val_acc: 0.5309
Epoch 6/110
782/782 [==============================] - 1148s 1s/step - loss: 1.2427 - acc: 0.5564 - val_loss: 1.2568 - val_acc: 0.5734
Epoch 7/110
782/782 [==============================] - 1338s 2s/step - loss: 1.1905 - acc: 0.5782 - val_loss: 1.1812 - val_acc: 0.5933
Epoch 8/110
782/782 [==============================] - 1389s 2s/step - loss: 1.1367 - acc: 0.5956 - val_loss: 1.1263 - val_acc: 0.6145
Epoch 9/110
782/782 [==============================] - 1377s 2s/step - loss: 1.1073 - acc: 0.6094 - val_loss: 1.1759 - val_acc: 0.6045
Epoch 10/110
782/782 [==============================] - 1379s 2s/step - loss: 1.0762 - acc: 0.6213 - val_loss: 0.9776 - val_acc: 0.6592
Epoch 11/110
782/782 [==============================] - 1225s 2s/step - loss: 1.0434 - acc: 0.6326 - val_loss: 1.0326 - val_acc: 0.6422
Epoch 12/110
782/782 [==============================] - 1181s 2s/step - loss: 1.0125 - acc: 0.6420 - val_loss: 0.9152 - val_acc: 0.6812
Epoch 13/110
782/782 [==============================] - 2467s 3s/step - loss: 0.9838 - acc: 0.6554 - val_loss: 0.9542 - val_acc: 0.6713
Epoch 14/110
782/782 [==============================] - 1258s 2s/step - loss: 0.9672 - acc: 0.6628 - val_loss: 1.0046 - val_acc: 0.6611
Epoch 15/110
782/782 [==============================] - 1342s 2s/step - loss: 0.9382 - acc: 0.6709 - val_loss: 0.9772 - val_acc: 0.6755
Epoch 16/110
782/782 [==============================] - 1280s 2s/step - loss: 0.9223 - acc: 0.6779 - val_loss: 0.9838 - val_acc: 0.6680
Epoch 17/110
782/782 [==============================] - 1342s 2s/step - loss: 0.9073 - acc: 0.6819 - val_loss: 0.7794 - val_acc: 0.7248
Epoch 18/110
782/782 [==============================] - 1376s 2s/step - loss: 0.8851 - acc: 0.6911 - val_loss: 0.8446 - val_acc: 0.7116
Epoch 19/110
782/782 [==============================] - 1439s 2s/step - loss: 0.8745 - acc: 0.6964 - val_loss: 0.8917 - val_acc: 0.6961
Epoch 20/110
782/782 [==============================] - 1400s 2s/step - loss: 0.8576 - acc: 0.7029 - val_loss: 0.7636 - val_acc: 0.7435
Epoch 21/110
782/782 [==============================] - 1246s 2s/step - loss: 0.8416 - acc: 0.7082 - val_loss: 0.7451 - val_acc: 0.7395
Epoch 22/110
782/782 [==============================] - 1354s 2s/step - loss: 0.8356 - acc: 0.7105 - val_loss: 0.7328 - val_acc: 0.7474
Epoch 23/110
782/782 [==============================] - 1195s 2s/step - loss: 0.8185 - acc: 0.7154 - val_loss: 0.8335 - val_acc: 0.7247
Epoch 24/110
782/782 [==============================] - 1158s 1s/step - loss: 0.8142 - acc: 0.7196 - val_loss: 0.7848 - val_acc: 0.7348
Epoch 25/110
782/782 [==============================] - 4057s 5s/step - loss: 0.7975 - acc: 0.7254 - val_loss: 0.8153 - val_acc: 0.7310
Epoch 26/110
782/782 [==============================] - 1135s 1s/step - loss: 0.7996 - acc: 0.7254 - val_loss: 0.7670 - val_acc: 0.7379
Epoch 27/110
782/782 [==============================] - 1267s 2s/step - loss: 0.7970 - acc: 0.7280 - val_loss: 0.7382 - val_acc: 0.7478
Epoch 28/110
782/782 [==============================] - 1380s 2s/step - loss: 0.7820 - acc: 0.7345 - val_loss: 0.6860 - val_acc: 0.7598
Epoch 29/110
782/782 [==============================] - 1125s 1s/step - loss: 0.7759 - acc: 0.7343 - val_loss: 0.7586 - val_acc: 0.7403
Epoch 30/110
782/782 [==============================] - 1136s 1s/step - loss: 0.7641 - acc: 0.7395 - val_loss: 0.6756 - val_acc: 0.7642
Epoch 31/110
782/782 [==============================] - 1148s 1s/step - loss: 0.7613 - acc: 0.7394 - val_loss: 0.7085 - val_acc: 0.7605
Epoch 32/110
782/782 [==============================] - 1131s 1s/step - loss: 0.7505 - acc: 0.7445 - val_loss: 0.7100 - val_acc: 0.7591
Epoch 33/110
782/782 [==============================] - 1134s 1s/step - loss: 0.7469 - acc: 0.7430 - val_loss: 0.7521 - val_acc: 0.7498
Epoch 34/110
782/782 [==============================] - 1143s 1s/step - loss: 0.7454 - acc: 0.7484 - val_loss: 0.7058 - val_acc: 0.7632
Epoch 35/110
782/782 [==============================] - 1123s 1s/step - loss: 0.7338 - acc: 0.7505 - val_loss: 0.6260 - val_acc: 0.7888
Epoch 36/110
782/782 [==============================] - 1134s 1s/step - loss: 0.7365 - acc: 0.7484 - val_loss: 0.6839 - val_acc: 0.7697
Epoch 37/110
782/782 [==============================] - 1136s 1s/step - loss: 0.7210 - acc: 0.7531 - val_loss: 0.6919 - val_acc: 0.7706
Epoch 38/110
782/782 [==============================] - 1132s 1s/step - loss: 0.7204 - acc: 0.7532 - val_loss: 0.6290 - val_acc: 0.7877
Epoch 39/110
782/782 [==============================] - 18218s 23s/step - loss: 0.7164 - acc: 0.7568 - val_loss: 0.6181 - val_acc: 0.7928
Epoch 40/110
782/782 [==============================] - 1237s 2s/step - loss: 0.7069 - acc: 0.7604 - val_loss: 0.6327 - val_acc: 0.7862
Epoch 41/110
782/782 [==============================] - 1236s 2s/step - loss: 0.7049 - acc: 0.7612 - val_loss: 0.6265 - val_acc: 0.7869
Epoch 42/110
782/782 [==============================] - 1142s 1s/step - loss: 0.7006 - acc: 0.7639 - val_loss: 0.6594 - val_acc: 0.7762
Epoch 43/110
782/782 [==============================] - 1117s 1s/step - loss: 0.6990 - acc: 0.7666 - val_loss: 0.6604 - val_acc: 0.7780
Epoch 44/110
782/782 [==============================] - 1124s 1s/step - loss: 0.6978 - acc: 0.7645 - val_loss: 0.6582 - val_acc: 0.7818
Epoch 45/110
782/782 [==============================] - 1124s 1s/step - loss: 0.6902 - acc: 0.7681 - val_loss: 0.6572 - val_acc: 0.7804
Epoch 46/110
782/782 [==============================] - 1121s 1s/step - loss: 0.6870 - acc: 0.7698 - val_loss: 0.5851 - val_acc: 0.8029
Epoch 47/110
782/782 [==============================] - 1121s 1s/step - loss: 0.6807 - acc: 0.7696 - val_loss: 0.5823 - val_acc: 0.8019
Epoch 48/110
782/782 [==============================] - 1120s 1s/step - loss: 0.6803 - acc: 0.7716 - val_loss: 0.7111 - val_acc: 0.7674
Epoch 49/110
782/782 [==============================] - 1124s 1s/step - loss: 0.6767 - acc: 0.7728 - val_loss: 0.6012 - val_acc: 0.8017
Epoch 50/110
782/782 [==============================] - 1127s 1s/step - loss: 0.6678 - acc: 0.7758 - val_loss: 0.6249 - val_acc: 0.7932
Epoch 51/110
782/782 [==============================] - 2816s 4s/step - loss: 0.6702 - acc: 0.7759 - val_loss: 0.5816 - val_acc: 0.8057
Epoch 52/110
782/782 [==============================] - 1353s 2s/step - loss: 0.6635 - acc: 0.7779 - val_loss: 0.5656 - val_acc: 0.8061
Epoch 53/110
782/782 [==============================] - 1409s 2s/step - loss: 0.6599 - acc: 0.7781 - val_loss: 0.6332 - val_acc: 0.7899
Epoch 54/110
782/782 [==============================] - 1341s 2s/step - loss: 0.6543 - acc: 0.7803 - val_loss: 0.5927 - val_acc: 0.8010
Epoch 55/110
782/782 [==============================] - 785s 1s/step - loss: 0.6534 - acc: 0.7818 - val_loss: 0.5907 - val_acc: 0.8019
Epoch 56/110
782/782 [==============================] - 644s 823ms/step - loss: 0.6621 - acc: 0.7785 - val_loss: 0.5737 - val_acc: 0.8067
Epoch 57/110
782/782 [==============================] - 606s 775ms/step - loss: 0.6511 - acc: 0.7832 - val_loss: 0.5637 - val_acc: 0.8134
Epoch 58/110
782/782 [==============================] - 618s 790ms/step - loss: 0.6491 - acc: 0.7835 - val_loss: 0.5755 - val_acc: 0.8036
Epoch 59/110
782/782 [==============================] - 644s 824ms/step - loss: 0.6456 - acc: 0.7842 - val_loss: 0.5879 - val_acc: 0.7984
Epoch 60/110
782/782 [==============================] - 622s 795ms/step - loss: 0.6394 - acc: 0.7848 - val_loss: 0.6329 - val_acc: 0.7915
Epoch 61/110
782/782 [==============================] - 605s 774ms/step - loss: 0.6342 - acc: 0.7856 - val_loss: 0.5705 - val_acc: 0.8082
Epoch 62/110
782/782 [==============================] - 603s 771ms/step - loss: 0.6328 - acc: 0.7885 - val_loss: 0.5848 - val_acc: 0.8056
Epoch 63/110
782/782 [==============================] - 607s 777ms/step - loss: 0.6302 - acc: 0.7898 - val_loss: 0.6110 - val_acc: 0.8019
Epoch 64/110
782/782 [==============================] - 635s 812ms/step - loss: 0.6315 - acc: 0.7892 - val_loss: 0.5731 - val_acc: 0.8106
Epoch 65/110
782/782 [==============================] - 676s 865ms/step - loss: 0.6223 - acc: 0.7920 - val_loss: 0.5528 - val_acc: 0.8157
Epoch 66/110
782/782 [==============================] - 772s 987ms/step - loss: 0.6211 - acc: 0.7900 - val_loss: 0.5523 - val_acc: 0.8183
Epoch 67/110
782/782 [==============================] - 660s 843ms/step - loss: 0.6274 - acc: 0.7909 - val_loss: 0.5513 - val_acc: 0.8162
Epoch 68/110
782/782 [==============================] - 663s 847ms/step - loss: 0.6177 - acc: 0.7934 - val_loss: 0.5602 - val_acc: 0.8112
Epoch 69/110
782/782 [==============================] - 613s 783ms/step - loss: 0.6174 - acc: 0.7933 - val_loss: 0.5611 - val_acc: 0.8162
Epoch 70/110
782/782 [==============================] - 595s 761ms/step - loss: 0.6154 - acc: 0.7950 - val_loss: 0.5177 - val_acc: 0.8271
Epoch 71/110
782/782 [==============================] - 616s 788ms/step - loss: 0.6098 - acc: 0.7972 - val_loss: 0.5667 - val_acc: 0.8075
Epoch 72/110
782/782 [==============================] - 629s 805ms/step - loss: 0.6029 - acc: 0.7981 - val_loss: 0.5692 - val_acc: 0.8122
Epoch 73/110
782/782 [==============================] - 619s 792ms/step - loss: 0.6065 - acc: 0.7962 - val_loss: 0.5462 - val_acc: 0.8161
Epoch 74/110
782/782 [==============================] - 664s 849ms/step - loss: 0.6003 - acc: 0.7988 - val_loss: 0.5065 - val_acc: 0.8280
Epoch 75/110
782/782 [==============================] - 635s 812ms/step - loss: 0.6006 - acc: 0.7972 - val_loss: 0.5207 - val_acc: 0.8232
Epoch 76/110
782/782 [==============================] - 644s 823ms/step - loss: 0.5966 - acc: 0.8008 - val_loss: 0.5240 - val_acc: 0.8235
Epoch 77/110
782/782 [==============================] - 635s 812ms/step - loss: 0.5950 - acc: 0.8010 - val_loss: 0.5568 - val_acc: 0.8130
Epoch 78/110
782/782 [==============================] - 638s 816ms/step - loss: 0.5938 - acc: 0.8021 - val_loss: 0.5413 - val_acc: 0.8195
Epoch 79/110
782/782 [==============================] - 658s 841ms/step - loss: 0.5900 - acc: 0.8037 - val_loss: 0.5213 - val_acc: 0.8256
Epoch 80/110
782/782 [==============================] - 645s 825ms/step - loss: 0.5908 - acc: 0.8020 - val_loss: 0.5219 - val_acc: 0.8252
Epoch 81/110
782/782 [==============================] - 627s 801ms/step - loss: 0.5937 - acc: 0.8002 - val_loss: 0.5811 - val_acc: 0.8093
Epoch 82/110
782/782 [==============================] - 640s 818ms/step - loss: 0.5878 - acc: 0.8023 - val_loss: 0.5490 - val_acc: 0.8189
Epoch 83/110
782/782 [==============================] - 651s 833ms/step - loss: 0.5837 - acc: 0.8045 - val_loss: 0.5100 - val_acc: 0.8337
Epoch 84/110
782/782 [==============================] - 2579s 3s/step - loss: 0.5814 - acc: 0.8038 - val_loss: 0.5173 - val_acc: 0.8269
Epoch 85/110
782/782 [==============================] - 829s 1s/step - loss: 0.5846 - acc: 0.8066 - val_loss: 0.5214 - val_acc: 0.8258
Epoch 86/110
782/782 [==============================] - 805s 1s/step - loss: 0.5786 - acc: 0.8079 - val_loss: 0.5060 - val_acc: 0.8307
Epoch 87/110
782/782 [==============================] - 741s 948ms/step - loss: 0.5848 - acc: 0.8054 - val_loss: 0.5130 - val_acc: 0.8272
Epoch 88/110
782/782 [==============================] - 726s 929ms/step - loss: 0.5741 - acc: 0.8084 - val_loss: 0.5200 - val_acc: 0.8253
Epoch 89/110
782/782 [==============================] - 837s 1s/step - loss: 0.5704 - acc: 0.8093 - val_loss: 0.5360 - val_acc: 0.8208
Epoch 90/110
782/782 [==============================] - 835s 1s/step - loss: 0.5701 - acc: 0.8092 - val_loss: 0.4931 - val_acc: 0.8324
Epoch 91/110
782/782 [==============================] - 647s 827ms/step - loss: 0.5672 - acc: 0.8094 - val_loss: 0.5290 - val_acc: 0.8226
Epoch 92/110
782/782 [==============================] - 640s 818ms/step - loss: 0.5682 - acc: 0.8113 - val_loss: 0.4938 - val_acc: 0.8360
Epoch 93/110
782/782 [==============================] - 640s 818ms/step - loss: 0.5607 - acc: 0.8127 - val_loss: 0.4986 - val_acc: 0.8328
Epoch 94/110
782/782 [==============================] - 699s 893ms/step - loss: 0.5664 - acc: 0.8120 - val_loss: 0.4892 - val_acc: 0.8378
Epoch 95/110
782/782 [==============================] - 737s 943ms/step - loss: 0.5670 - acc: 0.8109 - val_loss: 0.4919 - val_acc: 0.8356
Epoch 96/110
782/782 [==============================] - 704s 900ms/step - loss: 0.5595 - acc: 0.8113 - val_loss: 0.4765 - val_acc: 0.8393
Epoch 97/110
782/782 [==============================] - 645s 825ms/step - loss: 0.5621 - acc: 0.8110 - val_loss: 0.4988 - val_acc: 0.8354
Epoch 98/110
782/782 [==============================] - 641s 820ms/step - loss: 0.5613 - acc: 0.8134 - val_loss: 0.5074 - val_acc: 0.8326
Epoch 99/110
782/782 [==============================] - 641s 820ms/step - loss: 0.5561 - acc: 0.8130 - val_loss: 0.4858 - val_acc: 0.8380
Epoch 100/110
782/782 [==============================] - 735s 940ms/step - loss: 0.5550 - acc: 0.8160 - val_loss: 0.4860 - val_acc: 0.8377
Epoch 101/110
782/782 [==============================] - 762s 974ms/step - loss: 0.5579 - acc: 0.8140 - val_loss: 0.5294 - val_acc: 0.8241
Epoch 102/110
782/782 [==============================] - 755s 965ms/step - loss: 0.5524 - acc: 0.8157 - val_loss: 0.4852 - val_acc: 0.8397
Epoch 103/110
782/782 [==============================] - 668s 854ms/step - loss: 0.5520 - acc: 0.8156 - val_loss: 0.5160 - val_acc: 0.8297
Epoch 104/110
782/782 [==============================] - 714s 914ms/step - loss: 0.5431 - acc: 0.8174 - val_loss: 0.4870 - val_acc: 0.8368
Epoch 105/110
782/782 [==============================] - 711s 909ms/step - loss: 0.5516 - acc: 0.8157 - val_loss: 0.5069 - val_acc: 0.8321
Epoch 106/110
782/782 [==============================] - 711s 909ms/step - loss: 0.5403 - acc: 0.8183 - val_loss: 0.4977 - val_acc: 0.8396
Epoch 107/110
782/782 [==============================] - 738s 944ms/step - loss: 0.5494 - acc: 0.8182 - val_loss: 0.5016 - val_acc: 0.8360
Epoch 108/110
782/782 [==============================] - 884s 1s/step - loss: 0.5428 - acc: 0.8177 - val_loss: 0.5092 - val_acc: 0.8326
Epoch 109/110
782/782 [==============================] - 794s 1s/step - loss: 0.5438 - acc: 0.8183 - val_loss: 0.4853 - val_acc: 0.8390
Epoch 110/110
782/782 [==============================] - 759s 971ms/step - loss: 0.5405 - acc: 0.8198 - val_loss: 0.5157 - val_acc: 0.8267

score = model.evaluate(trainX, trainy, batch_size=BATCH_SIZE)
print('Train Loss:', score[0])
print('Train Accuracy:', score[1])
50000/50000 [==============================] - 202s 4ms/step
Train Loss: 0.393458710403
Train Accuracy: 0.8659

score = model.evaluate(testX, testy, batch_size=64)
print('Train Loss:', score[0])
print('Train Accuracy:', score[1])
10000/10000 [==============================] - 39s 4ms/step
Train Loss: 0.515699672985
Train Accuracy: 0.8267

ypred = model.predict(testX)
print(model.predict_classes(testX))

print(ypred)

ypred = np.argmax(ypred, axis=1)
print(ypred)
print(confusion_matrix(np.argmax(testy,axis=1), ypred))
[3 8 8 ..., 5 1 7]
[[  7.88541183e-06   9.98514952e-05   1.69038994e-03 ...,   3.09805978e-06
    4.76431660e-03   2.94648897e-04]
 [  7.19147429e-05   2.05559234e-04   5.40538371e-13 ...,   7.73625896e-16
    9.99718964e-01   3.54215513e-06]
 [  3.39024933e-03   7.56004220e-03   1.97599137e-09 ...,   3.41495894e-11
    9.87986505e-01   1.06321217e-03]
 ..., 
 [  5.58043577e-11   7.44339568e-09   4.67916243e-06 ...,   1.30046726e-06
    4.19388346e-09   1.47872692e-08]
 [  4.77853987e-08   9.99966383e-01   1.68982287e-13 ...,   2.91245462e-13
    1.49664992e-08   3.36660705e-05]
 [  6.49424145e-13   2.25068445e-14   1.20074228e-11 ...,   9.99931693e-01
    2.53155278e-16   7.98354438e-13]]
[3 8 8 ..., 5 1 7]
[[853   9  26  13  14   0  11   5  37  32]
 [  3 900   0   2   2   0   4   0  14  75]
 [ 40   0 752  16  68  17  80  15   4   8]
 [  9   3  49 587  83  55 155  21  14  24]
 [  3   0  24  10 865   2  72  19   4   1]
 [  6   1  33 144  62 611  86  42   1  14]
 [  2   1  10   8   8   1 965   1   2   2]
 [  9   2  19  18  53   2  15 865   1  16]
 [ 29   8   8   3   1   0  11   0 923  17]
 [ 11  13   1   1   1   1   7   2  17 946]]
model.save("D:\\cifar-10_imageclassification\\cifar_model.h5")
